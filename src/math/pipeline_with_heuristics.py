import os
import json
import glob
import re
from typing import Dict, List, Tuple, Set

import jsonlines
import pickle
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

import torch
from transformers import DPRContextEncoder, DPRContextEncoderTokenizer

from src.utils.model_factory import Model

# Initialize the LLM model
# Replace 'gpt-3.5-turbo' with your actual model identifier if different
llm_model = Model('gpt-3.5-turbo')
# llm_model = Model('gpt-4o-mini')
# llm_model = Model('gpt-4o')

# Initialize the DPR model for embedding
dpr_model_name = 'facebook/dpr-ctx_encoder-single-nq-base'
dpr_tokenizer = DPRContextEncoderTokenizer.from_pretrained(dpr_model_name)
dpr_model = DPRContextEncoder.from_pretrained(dpr_model_name)
dpr_model.eval()

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
dpr_model.to(device)

if torch.cuda.device_count() > 1:
    print(f"Using {torch.cuda.device_count()} GPUs for DPR embedding!")
    dpr_model = torch.nn.DataParallel(dpr_model)

def load_json_file(file_path: str) -> dict:
    """Load a JSON file and return its content as a dictionary."""
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            data = json.load(file)
        return data
    except json.JSONDecodeError as e:
        print(f"Error decoding JSON for file {file_path}: {e}")
        return {}
    except Exception as e:
        print(f"Unexpected error loading file {file_path}: {e}")
        return {}

def load_heuristics_embeddings(embeddings_file_path: str) -> Tuple[List[str], np.ndarray]:
    """
    Load heuristics and their embeddings from a pickle file.

    Args:
        embeddings_file_path (str): Path to the embeddings pickle file.

    Returns:
        Tuple[List[str], np.ndarray]: A tuple containing the list of heuristics and their embeddings.
    """
    try:
        with open(embeddings_file_path, 'rb') as f_emb:
            data = pickle.load(f_emb)
        heuristics = data['heuristics']
        embeddings = data['embeddings']
        print(f"Loaded {len(heuristics)} heuristics and their embeddings from '{embeddings_file_path}'.")
        return heuristics, embeddings
    except FileNotFoundError:
        print(f"Embeddings file not found at '{embeddings_file_path}'. Exiting.")
        exit(1)
    except Exception as e:
        print(f"Error loading embeddings from '{embeddings_file_path}': {e}")
        exit(1)

def embed_text(text: str, max_length: int = 512) -> np.ndarray:
    """
    Embed a single text string using DPR.

    Args:
        text (str): The text to embed.
        max_length (int, optional): Maximum token length. Defaults to 512.

    Returns:
        np.ndarray: The embedding vector.
    """
    inputs = dpr_tokenizer(
        [text],
        padding=True,
        truncation=True,
        return_tensors='pt',
        max_length=max_length
    )
    inputs = {k: v.to(device) for k, v in inputs.items()}

    with torch.no_grad():
        outputs = dpr_model(**inputs)
        embedding = outputs.pooler_output.cpu().numpy().astype('float16')  # Shape: (1, hidden_size)

    return embedding[0]

def get_top_k_heuristics(query: str, heuristics: List[str], embeddings: np.ndarray, k: int = 5) -> List[Tuple[str, float]]:
    """
    Retrieve the top K most relevant heuristics based on the query.

    Args:
        query (str): The input query string.
        heuristics (List[str]): List of heuristics.
        embeddings (np.ndarray): Numpy array of heuristic embeddings.
        k (int, optional): Number of top heuristics to retrieve. Defaults to 5.

    Returns:
        List[Tuple[str, float]]: A list of tuples containing the heuristic and its similarity score.
    """
    query_embedding = embed_text(query).reshape(1, -1)  # Shape: (1, hidden_size)

    # Compute cosine similarity
    similarities = cosine_similarity(query_embedding, embeddings)[0]  # Shape: (num_heuristics,)

    # Get top K indices
    top_k_indices = similarities.argsort()[-k:][::-1]

    top_k_heuristics = [(heuristics[idx], float(similarities[idx])) for idx in top_k_indices]

    return top_k_heuristics

def generate_llm_solution(problem: str, heuristics: List[str] = None) -> str:
    """
    Generate a solution to the problem using the LLM, optionally with provided heuristics.

    Args:
        problem (str): The problem statement.
        heuristics (List[str], optional): A list of heuristics to include in the prompt. Defaults to None.

    Returns:
        str: The solution generated by the LLM.
    """
    if heuristics:
        # Prepare heuristics string
        heuristics_str = "\n".join([f"{idx + 1}. {heuristic}" for idx, heuristic in enumerate(heuristics)])

        # Construct the prompt with heuristics included
        prompt = (
            "You are an AI assistant that solves mathematical problems:\n\n"
            f"Here are some useful tips that you might find helpful. Some or all of them may be irrelevant to the given problem. {heuristics_str}\n\n"
            "Solve the following problem using the above heuristics. Provide a detailed solution.\n"
            "Ensure that the final answer is enclosed within LaTeX box formatting like \\boxed{...}.\n\n"
            f"Problem: {problem}"
            f"Think step by step"
        )
    else:
        # Construct the prompt without heuristics
        prompt = (
            f"Solve the following problem and provide a detailed solution.\n"
            f"Ensure that the final answer is enclosed within LaTeX box formatting like \\boxed{{...}}.\n\n"
            f"Problem: {problem}"
            f"Think step by step"
        )

    response = llm_model.get_response(prompt)
    return response.strip()

def extract_boxed_answer(solution: str) -> str:
    """
    Extracts the content within \boxed{...} from a solution.

    Args:
        solution (str): The full solution text.

    Returns:
        str: The content inside \boxed{...}, or an empty string if not found.
    """
    match = re.search(r'\\boxed\{([^}]+)\}', solution)
    return match.group(1).strip() if match else ""

def evaluate_correctness(provided_solution: str, llm_solution: str, heuristics: List[str]) -> str:
    """
    Use the LLM to evaluate whether the provided solution and the LLM-generated solution are correct and equivalent.
    Allows different formatting if the answers are conceptually the same.

    Args:
        provided_solution (str): The solution provided in the JSON file.
        llm_solution (str): The solution generated by the LLM.
        heuristics (List[str]): A list of general heuristics to include in the prompt.

    Returns:
        str: 'Equivalent', 'Incorrect', or 'Cannot Determine'.
    """
    # Extract boxed answers
    provided_boxed = extract_boxed_answer(provided_solution)
    llm_boxed = extract_boxed_answer(llm_solution)

    # Debugging: Print extracted boxed answers
    print(f"    Provided boxed answer: {provided_boxed}")
    print(f"    LLM boxed answer: {llm_boxed}")

    # Check if both boxed answers are present
    if not provided_boxed or not llm_boxed:
        print("    Missing boxed answer in provided or LLM solution.")
        return "Cannot Determine"

    # Prepare evaluation prompt for LLM
    evaluation_prompt = (
        "You are an AI assistant that evaluates the correctness of mathematical solutions based on the following heuristics:\n\n"
        "Here are two final answers enclosed within \\boxed{...}. Determine if they are mathematically equivalent.\n\n"
        f"Answer 1: \\boxed{{{provided_boxed}}}\n\n"
        f"Answer 2: \\boxed{{{llm_boxed}}}\n\n"
        "Consider different representations (e.g., fractions vs. decimals) as equivalent if they represent the same value.\n"
        "Respond with only one of the following options exactly as stated and without any additional text:\n"
        "'Equivalent' - if both answers represent the same value.\n"
        "'Incorrect' - if the answers represent different values.\n"
        "'Cannot Determine' - if you cannot determine the equivalence based on the provided information."
    )
    # print(f"    Evaluation prompt:\n{evaluation_prompt}\n")

    # Get evaluation from LLM
    evaluation_response = llm_model.get_response(evaluation_prompt).strip().lower()

    # Map LLM response to standardized categories
    if "equivalent" in evaluation_response:
        return "Equivalent"
    elif "incorrect" in evaluation_response:
        return "Incorrect"
    else:
        # If the response is unclear or 'Cannot Determine'
        return "Cannot Determine"

def save_results_to_jsonl(output_path: str, results: List[Dict[str, str]]) -> None:
    """
    Save the evaluation results to a JSON Lines (JSONL) file.

    Args:
        output_path (str): Path to the JSONL file.
        results (List[Dict[str, str]]): List of result dictionaries.
    """
    try:
        with jsonlines.open(output_path, mode='w') as writer:
            for result in results:
                writer.write(result)
        print(f"Successfully saved results to '{output_path}'.")
    except Exception as e:
        print(f"Error saving results to '{output_path}': {e}")

def evaluate_solutions(folder_path: str, heuristics: List[str], embeddings: np.ndarray, max_problems: int) -> Tuple[int, int, float, int, int, float, List[Dict[str, str]]]:
    """
    Evaluate JSON files in a specific folder up to a maximum number of problems.
    Evaluates both with and without heuristics.

    Args:
        folder_path (str): Path to the category folder containing JSON files.
        heuristics (List[str]): A list of general heuristics to include in prompts.
        embeddings (np.ndarray): Numpy array of heuristic embeddings.
        max_problems (int): Maximum number of problems to evaluate in this folder.

    Returns:
        Tuple[int, int, float, int, int, float, List[Dict[str, str]]]: 
            - Number of correct solutions without heuristics
            - Number of evaluated solutions without heuristics
            - Accuracy percentage without heuristics
            - Number of correct solutions with heuristics
            - Number of evaluated solutions with heuristics
            - Accuracy percentage with heuristics
            - List of result dictionaries
    """
    results = []
    correct_without = 0
    evaluated_without = 0
    correct_with = 0
    evaluated_with = 0

    # Find all JSON files in the folder
    json_files = glob.glob(os.path.join(folder_path, '*.json'))

    # Limit to max_problems
    json_files = json_files[:max_problems]

    print(f"Found {len(json_files)} JSON files in '{folder_path}'.")

    for idx, file_path in enumerate(json_files, 1):
        relative_file_path = os.path.relpath(file_path, folder_path)
        print(f"  Processing file {idx}/{len(json_files)}: {relative_file_path}")

        try:
            data = load_json_file(file_path)
            problem = data.get("problem", "").strip()
            provided_solution = data.get("solution", "").strip()

            if not problem or not provided_solution:
                print(f"    Skipping due to missing problem or solution.\n")
                continue

            # Step 1: Generate solution without heuristics
            initial_solution = generate_llm_solution(problem)
            print(f"    Initial solution generated without heuristics.")

            # Step 2: Evaluate correctness of initial solution
            correctness_without = evaluate_correctness(provided_solution, initial_solution, heuristics)
            print(f"    Correctness without heuristics: {correctness_without}")

            if correctness_without == "Equivalent":
                correct_without += 1
            if correctness_without in ["Equivalent", "Incorrect"]:
                evaluated_without += 1  # Only count if evaluation was possible

            # Step 3: Embed the initial solution and retrieve top K heuristics
            top_k = get_top_k_heuristics(initial_solution, heuristics, embeddings, k=5)
            top_k_heuristics = [heuristic for heuristic, score in top_k]
            print(f"    Retrieved top {len(top_k_heuristics)} heuristics based on initial solution.")

            # Step 4: Generate final solution with top K heuristics
            final_solution = generate_llm_solution(problem, heuristics=top_k_heuristics)
            print(f"    Final solution generated with top {len(top_k_heuristics)} heuristics.")

            # Step 5: Evaluate correctness of final solution
            correctness_with = evaluate_correctness(provided_solution, final_solution, heuristics)
            print(f"    Correctness with heuristics: {correctness_with}")

            if correctness_with == "Equivalent":
                correct_with += 1
            if correctness_with in ["Equivalent", "Incorrect"]:
                evaluated_with += 1  # Only count if evaluation was possible

            # Append the result
            results.append({
                'problem': problem,
                'provided_solution': provided_solution,
                'llm_initial_solution': initial_solution,
                'correctness_without_heuristics': correctness_without,
                'top_k_heuristics': top_k_heuristics,
                'llm_final_solution': final_solution,
                'correctness_with_heuristics': correctness_with
            })

            print(f"    Result without heuristics: {correctness_without}")
            print(f"    Result with heuristics: {correctness_with}\n")

        except Exception as e:
            print(f"    Error processing file {relative_file_path}: {e}\n")
            continue

    # Calculate accuracies
    accuracy_without = (correct_without / evaluated_without) * 100 if evaluated_without > 0 else 0.0
    accuracy_with = (correct_with / evaluated_with) * 100 if evaluated_with > 0 else 0.0

    return correct_without, evaluated_without, accuracy_without, correct_with, evaluated_with, accuracy_with, results

def main():
    # Configuration Parameters
    ROOT_TEST_FOLDER = "/home/cpp/jerryhuang/math/MATH/test/"  # Root folder containing subfolders
    CURRENT_WORKING_DIR = os.getcwd()  # Directory where the script is run from
    MAX_PROBLEMS_PER_FOLDER = 2  # Set to an integer to limit the number of problems per folder
    HEURISTICS_EMBEDDINGS_FILE = "/home/cpp/jerryhuang/reasoning/src/math/heuristics_embeddings.pkl"  # Path to heuristic embeddings

    # Step 1: Load heuristics and their embeddings
    heuristics, embeddings = load_heuristics_embeddings(HEURISTICS_EMBEDDINGS_FILE)

    # Traverse all subdirectories in ROOT_TEST_FOLDER
    for root, dirs, files in os.walk(ROOT_TEST_FOLDER):
        # Extract category name relative to ROOT_TEST_FOLDER
        category = os.path.relpath(root, ROOT_TEST_FOLDER)
        if category == '.':
            category = 'root'  # Files directly under ROOT_TEST_FOLDER, if any

        # Define the output JSONL file path in the current working directory
        # Replace any OS-specific path separators with underscores to avoid issues
        sanitized_category = category.replace(os.sep, '_')
        
        if category == 'number_theory':
            output_jsonl_filename = f"{sanitized_category}_heuristics_results.jsonl"
            output_jsonl_path = os.path.join(CURRENT_WORKING_DIR, output_jsonl_filename)

            print(f"Evaluating category: '{category}'")

            # Evaluate solutions in the current category folder
            correct_without, evaluated_without, accuracy_without, correct_with, evaluated_with, accuracy_with, results = evaluate_solutions(
                root, heuristics, embeddings, MAX_PROBLEMS_PER_FOLDER
            )

            # Save results to JSONL
            save_results_to_jsonl(output_jsonl_path, results)

            # Print accuracies for the current category
            print(f"  Accuracy without heuristics for '{category}': {correct_without}/{evaluated_without} = {accuracy_without:.2f}%")
            print(f"  Accuracy with heuristics for '{category}': {correct_with}/{evaluated_with} = {accuracy_with:.2f}%")
            print(f"  Results saved to: {output_jsonl_path}\n")

    print("Evaluation completed for all categories.")

if __name__ == "__main__":
    main()
